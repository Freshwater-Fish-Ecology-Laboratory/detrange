## ------ model stuff --------
1. default priors - should default priors capture what is expected by the manufacturer under 'standard' conditions - or be uninformative. in initial exploration using uninformative priors works best

2. transects nested within stations  - should we allow for multiple transects (directions) within stations? or should we treat all data from single station as one cluster? if no, should the degree (compass direction) of the transect be taken into account or should they simpy be used to estimate a 'typical' transect from each station? (if keep transect direction we could create detection rnage visualization not as sphere - but convex hull?)

3. model CPDI = Should we provide option to model n-shape curve, with low DE at close distance (i.e. CPDI; Kessler 2015).
should we model both and test best model on the data by DIC or model average estimates.

4. standardize - should i standradize distance covariate?

5. covariates - should we allow for covariates?
   - this will take some tweaking of the code to allow for dynamically creating model template code/term names etc.

6. I only have one fixed effect model currently (effect of station on slope, single intercept)
should I allow single slope and effect of station on interecept as well?
also should I allow mixed e.g. fixed effect of station on intercept, random effect of station on slope
with only one covariate (distance) this would lead to 2^3 (8) combinations (pooled, fixed, random for each of slope and intercept)

## ----- programming stuff -----
## coef reporting
should i report svalue in tidy() and predict() funs?

## plotting
should autoplot method for drfit object simply plot observed data or should it automatically generate predictions (takes a few seconds)?

## seed
how best to pass a seed into rjags and via several functions (e.g. dr_simulate)

## accepting null/character(0)
thoughts on accepting NULL/character(0) for function args and then doing if/else? (e.g. estimates function in universals)

## package names
is it better to have a 'suite' of R packages rather than one giant one to reduce dependencies? (i.e. shiny, eventually some kind fo simulation packages) - does this make it harder to publish? does it maek harder to document in articles/vignettes? i.e. one website for all to show workflows that use several packages?
does it maek sense to have one overarching package (drtools) to load/install them all?

## ------- simulation --------
## movement simulation
for movement simulation - should i use SiMRiv r package or use it as inspirationa dn build my own simulation functions to reduce dependencies?
simriv is a bit overkill - as we dont need multiple values of landscape resistance - only need 0 (water) and 1 (land)
simriv downsides
- depnds on raster and sp rather than sf and stars
- frequentist
- movement parameters approximated using 'heuristic approximation method' - not comparable to ML
If we can build a JAGS model to do the same thing, we can use sims package to simulate data
alternative is to use momentuHMM, but would prefer to code myself for dependency/flexibility reasons?

### use nimble?
- better support for simulation
- maybe can mimic jags by setting slice samplers - nimble not good at guessing init values on vague priors though
- users would have to download compiler and tools if not there: https://r-nimble.org/download
- c++ compilation time - need to check whether can pre-compile as in STAN


## ssdtools review
why prefix some functions with .?
why put ... + chk_unused(...) at end of method functions
can autoplot take function arguments?



